import json
import os
import re
import time
from argparse import ArgumentParser
import logging

import torch
from PIL import Image
from tqdm import tqdm
from transformers import AutoProcessor, AutoModelForCausalLM, GenerationConfig, set_seed

set_seed(45)

MAX_NEW_TOKENS = 150

# Model directory
MODEL_DIR = "/model-weights/Phi-4-multimodal-instruct"  # Local model path
HF_MODEL_ID = "microsoft/Phi-4-multimodal-instruct"  # Hugging Face Model ID

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

os.environ["HF_HOME"] = ""
os.environ["TRANSFORMERS_CACHE"] = ""

PROMPT = """Answer the question using one of the given choices based on the image.

Question:
{QUESTION}

Provide the response only in the following JSON format:
{{"Answer": "Must contain the correct letter along with corresponding text",
"Reasoning": "A brief explanation (max 80 words) based on the details in the image"}}

Always provide option including its corresponding text in "Answer". Do not provide any other extra information.
"""

def extract_answer_and_reason(text):
    """Extracts the answer and reasoning from the response generated by the model."""
    try:
        # Extract JSON from the response
        json_start = text.find('{')
        json_end = text.rfind('}') + 1
        if json_start != -1 and json_end != -1:
            json_str = text[json_start:json_end]
            data = json.loads(json_str)
            answer = data.get("Answer", "").strip()
            reasoning = data.get("Reasoning", "").strip()
            if answer and reasoning:
                return answer, reasoning
    except Exception as e:
        pass
    
    # If not in JSON format, use regex to extract answer and reasoning
    pattern = r'(?:\*\*Answer:\*\*|Answer:)\s*"?([^"\n]*)"?\s*(?:\*\*Reasoning:\*\*|Reasoning:)\s*"?([^"\n]*)"?'
    match = re.search(pattern, text)

    if match:
        return match.group(1), match.group(2)
    
    return text, None

def load_model(model_source="local"):
    """Load the Phi-4 model and processor from either local or Hugging Face."""
    logger.info(f"Loading Phi-4 Vision model from {'local directory' if model_source == 'local' else 'Hugging Face'}...")

    model_path = MODEL_DIR if model_source == "local" else HF_MODEL_ID

    # Load model
    model = AutoModelForCausalLM.from_pretrained(
        model_path, 
        trust_remote_code=True,  
        torch_dtype="auto",  
        device_map="auto",  
        _attn_implementation='eager'  # Default implementation
    )

    # Load processor
    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
    return model, processor

def resize_image(img_path, max_size=(350, 350)):
    """Resize the image to a maximum size while maintaining aspect ratio."""
    try:
        image = Image.open(img_path).convert("RGB")
        image.thumbnail(max_size, Image.LANCZOS)
        return image
    except Exception as e:
        logger.error(f"Error resizing image {img_path}: {e}")
        return None

def process_sample(model, processor, img_path, question, device):
    """Process a single image-question pair and generate a response."""
    try:
        # Resize image and prepare prompt
        image = resize_image(img_path)
        if image is None:
            return "Error: Could not process image"
        
        query_prompt = PROMPT.format(QUESTION=question)
        prompt = f"<|user|><|image_1|>\n{query_prompt}<|end|><|assistant|>"

        # Prepare inputs for the model
        inputs = processor(prompt, image, return_tensors="pt").to(device)

        # Generate the response
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS, 
                do_sample=False,
                pad_token_id=processor.tokenizer.pad_token_id,
                eos_token_id=processor.tokenizer.eos_token_id,
                bos_token_id=processor.tokenizer.bos_token_id,
                use_cache=True,
            )

        # Decode the response and return the result
        generated_ids = output[:, inputs['input_ids'].shape[1]:]
        predicted_answer = processor.batch_decode(
            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

        return predicted_answer if predicted_answer else "No answer generated"
    except Exception as e:
        logger.error(f"Error processing {img_path}: {e}")
        return "Error"

def evaluate(model, processor, dataset, image_folder, save_path, mode="single"):
    """Evaluate the model on the dataset and save the results."""
    results = []
    logger.info(f"Starting evaluation in {mode} mode...")
    intermediate_results_path = save_path.replace(".json", "_intermediate.json")
    prev_path = ""

    with tqdm(total=len(dataset), unit="sample") as pbar:
        for i, data in enumerate(dataset):
            img_path = os.path.join(image_folder, f"{data['ID']}.jpg")

            if not os.path.exists(img_path):
                logger.warning(f"Image not found for ID {data['ID']} at {img_path}")
                continue
            
            question_with_choices = data["Question"] + '\n' + data["Options"]
            response = process_sample(model, processor, img_path, question_with_choices, device)
            answer, reason = extract_answer_and_reason(response)

            results.append({
                "ID": data["ID"],
                "Question": question_with_choices,
                "Predicted_Answer": answer,
                "Predicted_Reasoning": reason,
                "Ground_Truth_Answer": data["Answer"],
                "Ground_Truth_Reasoning": data["Reasoning"],
                "Attribute": data["Attribute"],
            })

            # Save intermediate results every 10 samples
            if i % 10 == 0:
                intermediate_results_path = save_path.replace(".json", f"_intermediate_{i}_{time.strftime('%Y%m%d_%H%M%S')}.json")
                with open(intermediate_results_path, "w") as f:
                    json.dump(results, f, indent=4, default=str)
                if prev_path != "":
                    os.remove(prev_path)  
                logger.info(f"Intermediate results saved to {intermediate_results_path} and deleted {prev_path}.")
                prev_path = intermediate_results_path

            pbar.update(1)

    # Save final results
    with open(save_path, "w") as f:
        json.dump(results, f, indent=4, default=str)
    
    logger.info(f"Results saved to {save_path}.")

# Main execution
if __name__ == "__main__":
    start_time = time.time()

    # Command-line arguments
    parser = ArgumentParser()
    parser.add_argument("--dataset", type=str, default="./data/eval3/QA_Eval3.json", help="Path to dataset")
    parser.add_argument("--image_folder", type=str, default="./data/processed_images", help="Path to image folder")
    parser.add_argument("--device", type=str, default="cuda", help="Device to run the model on")
    parser.add_argument("--save_path", type=str, default="./results/results_Phi4.json", help="Output file to save results")
    parser.add_argument("--model_source", type=str, default="hf", help="Model source: 'local' or 'hf'")
    parser.add_argument("--mode", type=str, default="single", choices=["single", "batch"], help="Single or batch processing")

    args = parser.parse_args()
    
    # Define device
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')

    # Load model
    model, processor = load_model(args.model_source)
    model.to(device)

    # Load dataset
    with open(args.dataset, "r") as f:
        dataset = json.load(f)

    logger.info(f"Loaded dataset with {len(dataset)} samples.")

    # Run evaluation
    evaluate(model, processor, dataset, args.image_folder, args.save_path, args.mode)

    logger.info(f"Total time taken: {time.time() - start_time:.2f} seconds")
