<!DOCTYPE html>
<html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta property='og:title' content='HumaniBench: A Benchmark for Human-Centric Alignment in Multimodal Large Language Models' />
<meta property='og:image' content='' />
<meta property='og:description' content='' />
<meta property='og:url' content='https://github.com/' />
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<!-- TYPE BELOW IS PROBABLY: 'website' or 'article' or look on https://ogp.me/#types -->
<meta property="og:type" content='website' />

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9VZKE74FPW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-9VZKE74FPW');
  </script>
  <meta charset="utf-8">
  <meta name="description" content="HumaniBench: A Benchmark for Human-Centric Alignment in Multimodal Large Language Models">
  <meta name="keywords" content="
    HumaniBench, Multimodal Large Language Models, Human-Centric Alignment, Benchmark, Dataset, Evaluation, Fairness, Ethics, Perceptual Honesty, Multilingual Equity, Empathy">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HumaniBench: A Benchmark for Human-Centric Alignment in Multimodal Large Language Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">

</head>

<style>
  @import url('https://fonts.cdnfonts.com/css/menlo');
</style>


<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h1 class="title is-2 publication-title">VLDBench: <u>V</u>ision <u>L</u>anguage Models -->
              <!-- <u>D</u>isinformation Detection <u>Bench</u>mark -->
            <h1 class="title is-2 publication-title">HumaniBench: A Benchmark for Human-Centric Alignment in Multimodal Large Language Models
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://scholar.google.com/citations?user=chcz7RMAAAAJ&hl=en">Shaina Raza</a><sup>1*</sup>,</span>
              <span class="author-block"><a href="https://aravind-3105.github.io/">Aravind Narayanan</a><sup>1*</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=lEWvRbIAAAAJ&hl=en">Vahid
                  Reza Khazaie</a><sup>1</sup></span>
              <span class="author-block"><a
                  href="https://scholar.google.com/citations?hl=en&user=Gn1FIg4AAAAJ">Mukund Sayeeganesh Chettiar</a><sup>1</sup></span>
              <span class="author-block"><a
                    href="#">Amandeep Singh</a><sup>1</sup></span>
              <span class="author-block"><a href="https://ashmalvayani.github.io/">Ashmal Vayani</a><sup>2</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=o1XZyLMAAAAJ&hl=en">Deval Pandya</a><sup>1</sup>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Vector Institute for Artificial Intelligence</span>
              <span class="author-block"><sup>2</sup>University of Central Florida</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                </span>
                <span class="link-block">
                  <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
<!--                 <span class="link-block">
                  <a href="https://github.com/VectorInstitute" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
            
                <span class="link-block">
                  <a href="#bibtex" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-obp"></i>
                    </span>
                    <span>BibTex</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <p align="justify">
          As multimodal large language models (MLLMs) become increasingly integrated into our digital environments, their ability to align with human values, expectations, and ethics becomes critically important. We introduce <strong>HumaniBench</strong>, a minimalist yet comprehensive benchmark designed to evaluate MLLMs on their alignment with human-centric principles. Unlike traditional benchmarks that emphasize raw accuracy, HumaniBench probes a model’s fairness, ethical compliance, perceptual honesty, multilingual equity, and empathy. By presenting real-world, non-synthetic visual question answering tasks across diverse social contexts, HumaniBench offers a unique lens through which to assess how models interact with the nuances of human perception, identity, and culture. Our benchmark sets the stage for building MLLMs that are not just capable—but also conscientious and trustworthy.</p>
        <!-- <br> -->

        <div class="column">
          <div style="text-align:center;">
            <img src="static/images/Figure1.png" style="max-width:100%">
            <div class="content has-text-justified">
              <p align="justify"> <b> <span>Figure</span></b>:
                HumaniBench: end-to-end benchmark construction pipeline. The figure traces four stages: (a) Data Preparation raw news and socialimage streams are deduplicated, captioned, and tagged by a VLM, then verified by human reviewers; (b) Alignment Principles & Evaluation seven humancentric dimensions (Fairness, Ethical Compliance, Perceptual Honesty, Contextual Reasoning, Multilingual Equity, Empathy, Robustness) and the metrics reported on the right; (c) Labeling for Task multimodal inputs are autolabelled with GPT-4o and refined through iterative human feedback; (d) Task Setting the curated corpus is organized into seven evaluation tasks (T1-T7). Together these stages yield a human-centric benchmark for assessing modern MLLMs.
              </p>
            </div>
          </div>
        </div>

        <br><br>
      </div>
    </div>

  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <!-- Visual Effects. -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal large language models (MLLMs) have achieved impressive capabilities in vision and language understanding; however, they often struggle on human-centered criteria such as <strong>fairness</strong>, <strong>ethical compliance</strong>, <strong>personalization</strong>, and <strong>perceptual accuracy</strong>—meaning their alignment with real-world human needs, values, and expectations. We introduce a <strong>novel benchmark dataset</strong> explicitly designed to evaluate and guide MLLMs toward more 
            <em>human-aligned behavior</em>. Specifically, we assess models’ ability to avoid <strong>harmful biases</strong>, ethically handle sensitive content, adapt outputs to user contexts, and maintain <em>human-like perceptual honesty</em> and <em>uncertainty handling</em>. We outline rigorous evaluation protocols that go beyond accuracy, measuring <strong>bias</strong>, <strong>trustworthiness</strong>, and <strong>user-centric performance</strong>. Baseline experiments with state-of-the-art MLLMs (open-source and proprietary) reveal significant gaps: models show strong performance at core vision-language tasks but still exhibit notable <strong>biases</strong> and <strong>consistency issues</strong> in human-centric scenarios. By highlighting these limitations, our benchmark provides a tool for the community to develop and validate MLLMs that are not only intelligent, but also <em>ethical</em>, <em>fair</em>, and <em>aligned with human perception and preferences</em>.<br>
          </p>
        </div>
      </div>
    </div>


  </section>




  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"><em>HumaniBench</em> is the first unified, real-world benchmark for evaluating human-centric alignment in multimodal LLMs; expert-verified across seven ethical AI dimensions.</h2>


          <div class="content has-text-justified">
            <p>
            <h5> <b> Main contributions: </b></h5>
            <ol>
              <li><strong>Unified Human-Centric Benchmark:</strong> Introduces HumaniBench, the first real-world benchmark that systematically evaluates multimodal LLMs across seven dimensions of human-centered AI, including fairness, empathy, and perceptual honesty.</li>
  
              <li><strong>Expert-Guided, Real-World Dataset:</strong> Comprises 32K image–question pairs derived from diverse social contexts and curated through a semi-automated GPT-4o pipeline with expert human validation—avoiding synthetic data to ensure realism.</li>
              
              <li><strong>Ethical & Multilingual Evaluation:</strong> Benchmarks models across multilingual equity, ethical compliance, and social sensitivity, offering robust metrics beyond accuracy (e.g., hallucination, empathy, contextual relevance, bias).</li>
              
              <li><strong>Holistic Model Assessment:</strong> Evaluates both open-source and proprietary MLLMs, revealing key alignment gaps and highlighting trade-offs between raw performance and human-centered trustworthiness.</li>

            </ol>
            </p>
          </div>
        </div>
      </div>
      
      <!--/ Abstract. -->

    </div>
  </section>

  <section class="section">

    <!--/ Matting. -->
    <div class="container is-max-desktop">

      <!-- Latent space editing applications -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          
          <h2 class="title is-3 has-text-centered">HumaniBench Framework Overview</h2>

          <div class="content has-text-justified">
            <p>VLDBench is a comprehensive classification multimodal benchmark for disinformation detection in news articles. We categorized our data into 13 unique news categories by providing image-text pairs to GPT-4o.</p>
            <div class="content has-text-centered"><img src="static/images/Table1.png"  style="max-width:80%">
              <p class="content has-text-justified"> <b> <span>Table</span></b>: Comparison of existing MLLM benchmarks with respect to seven human-centric alignment dimensions: (Fairness = lack of
                bias or stereotypes across demographics), (Ethical Compliance = adherence to moral or policy guidelines), (Perceptual Honesty = accurate
                interpretation of visual content without hallucination), (Contextual Reasoning = multi-turn or context-rich understanding), (Multilingual
                Equity = consistent performance across languages), (Empathy = sensitive responses to emotional or interpersonal scenarios), (Robustness
                = resilience to input perturbations or distribution shifts). Columns are marked with ✓ if covered, ✗ if not, and ∼ if partially. The “HC”
                column indicates overall human-centric coverage, and “Data Source” shows whether images are real (R) or synthetic (S).</p>
            </div>
          </div>

          <div class="content has-text-centered">
            <img src="static/images/Figure0.png" style="max-width:50%">
            <p class="content has-text-justified"><b> <span>Figure</span></b>: Overview of HumaniBench's alignment principles and associated evaluation metrics. The benchmark assesses models across seven dimensions of human-centered AI, including fairness, ethics, perceptual honesty, reasoning, multilinguality, empathy, and robustness.
            </p><br>
          </div>


        </div>
      </div>

      <!--/ Matting. -->
      <div class="container is-max-desktop">

        <!-- Latent space editing applications -->
        <div class="columns is-centered">
          <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Annotation Pipeline</h2>
            <div class="content has-text-centered">
              <img src="static/images/Figure2.png">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Overview of the annotation process for HumaniBench
              </p>
            </div>


            <div class="content has-text-justified">
              <p align="justify"> 
                HumaniBench employs a multi-stage annotation pipeline combining automation and expert validation. Real-world images are first filtered for duplicates using CLIP embeddings, then tagged with captions and demographic labels via GPT-4o. These annotations are reviewed by domain experts to ensure accuracy, fairness, and appropriateness. Images are then clustered by attributes like age, race, and gender to support bias analysis. Final task-specific annotations are generated and verified, resulting in a high-quality dataset of 32K images with rich metadata across five social dimensions.
              </p>
          </div>


          <h2 class="title is-3 has-text-centered">Tasks Overview</h2>
            <div class="content has-text-centered">
              <img src="static/images/Table2.png" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Overview of HumaniBench Tasks Grouped by Alignment Dimension.</p>
            </div>
            

            <div class="content has-text-justified">
              <p align="justify"> 
                Each of the seven tasks in HumaniBench corresponds to one or more of the seven core human-centric principles that we defined and is designed to reflect realistic, complex, and diverse scenarios.
              </p>
              <ul>
                <li><strong>T1: Scene Understanding</strong><br>
                    Open-ended VQA task where models describe a visual scene with attention to social context. Evaluates fairness and potential biases in describing individuals based on appearance, such as gender, race, and age.
                </li>
              
                <li><strong>T2: Instance Identity</strong><br>
                    Requires models to identify the key person or object in an image and interpret their role or identity. Assesses culturally sensitive reasoning and accuracy across social contexts.
                </li>
              
                <li><strong>T3: Instance Attribute</strong><br>
                    Multiple-choice questions about object-specific visual traits (e.g., color, accessories, attire). Measures perceptual honesty, hallucination, and subtle detail recognition.
                </li>
              
                <li><strong>T4: Language & Culture</strong><br>
                    Multilingual VQA task featuring questions in 11 languages to test cross-lingual consistency and cultural reasoning. Targets equity in MLLM performance across high- and low-resource languages.
                </li>
              
                <li><strong>T5: Visual Grounding & Localization</strong><br>
                    Models are prompted to localize specific entities in images using bounding boxes. Tasks emphasize spatial reasoning and visual-text grounding accuracy.
                </li>
              
                <li><strong>T6: Human-Centered Emotion</strong><br>
                    Assesses the ability of models to rewrite factual captions into emotionally sensitive responses for crisis or distress scenarios. Focuses on empathy and tone appropriateness.
                </li>
              
                <li><strong>T7: Robustness & Stability</strong><br>
                    Evaluates how model responses change under image perturbations (e.g., blur, noise). Tests the consistency and resilience of MLLM outputs under stress conditions.
                </li>
              </ul>
              
          </div>

        </div>
      </div>
      <br><br>
      <div class="content has-text-centered">
        <h3 class="title is-4 has-text-justified">Multimodal Models under Evaluation</h3>
        <div class="content has-text-justified">
          <p align="justify"> For HumaniBench, we evaluate thirteen state-of-the-art open-source multimodal models (VLMs) across diverse human-alignment tasks. These models represent a range of vision encoders, language backbones, and fusion techniques. By focusing exclusively on vision-language models, HumaniBench provides a rigorous testbed for assessing human-centered reasoning, fairness, empathy, and robustness in real-world multimodal scenarios. </p>
          <!-- I want width to be 80% center aligned-->
          <table style="width:60%; margin: 0 auto; border-collapse: collapse; font-family: sans-serif;">
            <thead style="background-color: #f2f2f2;">
              <tr>
                <th style="padding: 8px; border-bottom: 2px solid #ccc; font-size: 1.1em; text-align: left;">
                  Vision-Language Models (VLMs)
                </th>
              </tr>
            </thead>
            <tbody>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B" target="_blank">CogVLM2-19B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/CohereLabs/aya-vision-8b" target="_blank">Cohere Aya Vision 8B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/deepseek-ai/deepseek-vl2-small" target="_blank">DeepSeek VL2 Small</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/THUDM/glm-4v-9b" target="_blank">GLM-4V-9B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/OpenGVLab/InternVL2_5-8B" target="_blank">InternVL2.5 8B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/deepseek-ai/Janus-Pro-7B" target="_blank">Janus-Pro-7B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct" target="_blank">LLaMA 3.2 11B Vision Instruct</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/llava-hf/llava-v1.6-vicuna-7b-hf" target="_blank">LLaVA-v1.6-Vicuna-7B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/allenai/Molmo-7B-D-0924" target="_blank">Molmo-7B</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/microsoft/phi-4" target="_blank">Phi 4</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/microsoft/Phi-3.5-vision-instruct" target="_blank">Phi 3.5 Vision Instruct</a></td></tr>
              <tr><td style="padding: 6px; border-bottom: 1px solid #e0e0e0;"><a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct" target="_blank">Qwen2.5-VL-7B Instruct</a></td></tr>
              <tr><td style="padding: 6px;"><a href="#" target="_blank">Gemma 3</a></td></tr>
            </tbody>
          </table>
          
        
        
      </div>

      <br><br>

      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">
          <h2 class="title is-3">Benchmarking Results on HumaniBench</h2>

          <div class="content has-text-justified">
            <p> We evaluated both open‐source and closed‐source MLLMs on HumaniBench using a variety of tasks (T1–T7). This section presents our main empirical findings and highlights key challenges for MLLMs. </p>
          </div>

          <h3 class="title is-4 has-text-justified">Social Perception and Contextual Reasoning Remain Challenging</h3>

          <div class="content has-text-centered">
            <div class="content has-text-centered">
              <img src="static/images/Figure3.png" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Comparative Analysis of Model Performance. (a) Task-specific rankings with aggregate scores (lower is better). (b) Bias patterns across evaluation dimensions. (c) Accuracy distribution across test scenarios. Heatmaps share common axes from Tasks 1-3.</p>
            </div>
            <p style="text-align: justify;">
              HumaniBench reveals that despite strong performance in core tasks, many models still struggle with
              <strong>fairness</strong>, <strong>perceptual bias</strong>, and <strong>reasoning</strong>—especially across sensitive attributes like race, gender, and occupation. As shown in the figure above, some top-performing models (e.g., Phi 4, CogVLM2-19B) achieve high accuracy while maintaining relatively low bias, suggesting that <em>accuracy–fairness trade-offs are not inevitable</em>. However, disparities in performance remain most pronounced in race and gender categories, highlighting the need for targeted mitigation strategies.
            </p>

            <div class="content has-text-centered">
              <img src="static/images/Figure4.png" style="max-width:60%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Correlation coefficients between closed-ended accuracy (multiple-choice VQA) and open-ended reasoning accuracy (model explanations) across various VQA models. Closed-source models (highlighted in distinct color) show stronger correlations, indicating better consistency between selected answers and explanatory reasoning.</p>
            </div>

            <p style="text-align: justify;">
              The above figure highlights reasoning consistency, showing that closed-source models like Gemini and GPT-4o demonstrate stronger correlations between multiple-choice answers and their generated reasoning. This reflects better internal coherence, while open-source models continue to show variability in <em>faithfulness</em>, <em>contextual relevance</em>, and <em>coherence</em>. Together, these findings emphasize that achieving holistic human alignment requires going beyond accuracy—incorporating fairness, 
              robustness, and reasoning fidelity.
            </p>

            <!-- <img src="static/images/zero-shot_performance.png" style="max-width:100%"> -->
             <!-- <img src="static/images/radar_plot_page-0001.jpg" style="max-width:80%"> -->
            <!-- <p style="text-align: justify;"> <b> <span>Figure</span></b>: Performance comparison of vision language models for disinformation detection across key metrics: precision, recall, F1 and accuracy, with different colors representing distinct metrics </p> -->
          </div>
          <!-- <br /> -->
          <h3 class="title is-4 has-text-justified">Multilingual Gaps Persist Across MLLMs</h3>
          <div class="content has-text-centered">
            <div class="content has-text-centered">
              <img src="static/images/Figure5.png" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Examples of multilingual evaluation across high-resource (Mandarin, Portuguese, Korean, French) and low-resource (Urdu, Tamil) languages.</p>
            </div>

            <p style="text-align: justify;">
              HumaniBench includes multilingual evaluation across both open-ended and closed-ended tasks. 
              We assess models on a mix of high-resource and low-resource languages to better understand disparities 
              in accuracy and bias. While performance on high-resource languages tends to be more consistent, 
              we observe persistent gaps in low-resource language support, highlighting ongoing challenges in achieving 
              equitable multilingual alignment.</p>
              <table style="width:60%; margin: 0 auto; border-collapse: collapse; font-family: sans-serif;">
                <thead style="background-color: #f2f2f2;">
                  <tr>
                    <th style="padding: 10px; border-bottom: 2px solid #ccc;">High-Resource Languages</th>
                    <th style="padding: 10px; border-bottom: 2px solid #ccc;">Low-Resource Languages</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">English (Reference)</td>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Urdu</td>
                  </tr>
                  <tr>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">French</td>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Persian</td>
                  </tr>
                  <tr>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Spanish</td>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Bengali</td>
                  </tr>
                  <tr>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Portuguese</td>
                    <td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Punjabi</td>
                  </tr>
                  <tr>
                    <td style="padding: 10px;">Mandarin</td>
                    <td style="padding: 10px;">Tamil</td>
                  </tr>
                  <tr>
                    <td style="padding: 10px;">Korean</td>
                  </tr>
                </tbody>
              </table>



          </div>

          <h3 class="title is-4 has-text-justified">MLLMs Underperform Traditional Detectors in Bounding Box Localization</h3>
          <div class="content has-text-centered">
            <p style="text-align: justify;">
              Despite advancements in spatial reasoning, <strong>HumaniBench</strong> results show that MLLMs still underperform traditional object detectors like <em>Faster R-CNN</em> in bounding box localization. Evaluated on a 500-image benchmark, most models struggled with precision. <strong>GPT-4o</strong> emerged as the strongest among MLLMs, achieving the highest mAP and IoU, though still lagging behind the supervised baseline.</p>
            <p style="text-align: justify;">
              These results are expected since most MLLMs lack direct supervision for box-level tasks. Their spatial reasoning emerges from vision–language alignment, not from dedicated detection heads. While traditional detectors use region proposal networks and spatial backbones, MLLMs often output coordinates through text generation, requiring post-processing and yielding less precise localization overall.
            </p>
          </div>

          <h3 class="title is-4 has-text-justified">Empathy vs. Factuality: How MLLMs Vary by Prompt</h3>
          <div class="content has-text-centered">
            <p style="text-align: justify;">
              A key challenge in real-world image captioning—especially for sensitive news content—is balancing 
              <strong>emotional resonance</strong> with <strong>factual accuracy</strong>. To explore this, HumaniBench prompts MLLMs to generate captions in two styles: <em>Simple</em> (neutral, factual) and <em>Emphatic</em> (emotionally attuned). Using linguistic analysis tools, we observe that different models exhibit distinct <strong>emotional signatures</strong>. 
            </p>
            <p style="text-align: justify;">
              For example, some models like Phi emphasize serious emotional tone, while others such as OpenAI's o1-mini favor uplifting or motivational expressions. These variations highlight the importance of <em>model selection and calibration</em>, especially when generating content for crisis response, public engagement, or emotionally sensitive contexts. Aligning the model’s output style with communication goals is crucial for responsible deployment.
            </p>

          </div>

          <h3 class="title is-4 has-text-justified"> Robustness Under Real-World Conditions is still a Challenge for MLLMs</h3>
          <div class="content has-text-centered">
            <p style="text-align: justify;">
              Despite impressive performance on clean datasets, HumaniBench reveals that many MLLMs remain vulnerable 
              to everyday visual distortions like <strong>noise</strong>, <strong>blur</strong>, <strong>occlusions</strong>, and <strong>compression artifacts</strong>. Even minor perturbations can significantly degrade model accuracy, especially in tasks involving social attributes such as <em>race, gender, or cultural symbols</em>.
            </p>
            <p style="text-align: justify;">
              These findings underscore the importance of designing MLLMs that are not just accurate in ideal settings, 
              but also <strong>robust to real-world conditions</strong>. Addressing this gap will require advances in architecture design, training objectives, and dataset diversity to ensure consistent interpretability across noisy, dynamic, and imperfect inputs typical of real-world scenarios.
            </p>


          </div>
          <div class="content has-text-centered">
            <img src="static/images/Table12.png" style="max-width:80%">
            <p style="text-align: justify;"> <span><b>Figure</b></span>: Qualitative Robustness Evaluation Across Different Perturbations using GPT-4o </p>
          </div>


          <h3 class="title is-4 has-text-justified"> CoT Prompting improves Accuracy and Reduces Bias</h3>
          <div class="content has-text-centered">
            <p style="text-align: justify;">
              HumaniBench results show that using <strong>Chain-of-Thought (CoT) prompting</strong> guides models through step-by-step reasoning—leads to <strong>consistent accuracy gains (2–4%)</strong> and <strong>lower bias</strong> in open-ended VQA tasks (T1, T2). By encouraging intermediate reasoning rather than direct answers, CoT prompts improve model transparency and decision-making clarity on complex, socially nuanced queries.
              While this suggests CoT may be a valuable tool for mitigating bias in reasoning-heavy tasks, further investigation is needed to fully understand the causal mechanisms behind these improvements.
            </p>

          </div>
        </div>
      </div>

      

      <div class="content has-text-justified">
        <!-- Center align  -->
        <div class="column is-full-width has-text-centered">
          <h2 class="title is-3">Conclusion</h2>
        </div>
        <!-- <h2 class="title is-4 has-text-centered">Conclusion</h2> -->
        <div class="content has-text-justified">
          <p>
            HumaniBench introduces a comprehensive, real-world benchmark for evaluating the human alignment of multimodal large language models (MLLMs) across fairness, empathy, multilingual equity, perceptual honesty, and robustness. Through seven diverse tasks and evaluations spanning over 30K curated image–question pairs, the benchmark reveals persistent gaps in social reasoning, bias mitigation, and consistency especially under perturbation and across low-resource languages. While some models show promising accuracy–fairness trade-offs and benefit from CoT prompting, the findings underscore the importance of going beyond accuracy toward building models that are not only capable but also ethically and contextually aware.</p>
          <br>
          <p>For additional details about HumaniBench evaluation and experimental results, please refer to our main paper. Thank you! </p>
        </div>

      
      </div>
    </div>



      

  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title"><a id="bibtex">BibTeX</a></h2>
      <pre><code>To be released.</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p class="has-text-centered">
            Website adapted from the following <a href="https://mingukkang.github.io/GigaGAN/">source code</a>.
          </p>
        </div>
      </div>
    </div>
    </div>
  </footer>


  <script src="juxtapose/js/juxtapose.js"></script>

  <script>
    var slider;
    let origOptions = {
      "makeResponsive": true,
      "showLabels": true,
      "mode": "horizontal",
      "showCredits": true,
      "animate": true,
      "startingPosition": "50"
    };

    const juxtaposeSelector = "#juxtapose-embed";
    const transientSelector = "#juxtapose-hidden";

    inputImage.src = "./static/images/".concat(name, "_input.jpg")
    outputImage.src = "./static/images/".concat(name, "_output.jpg")

    let images = [inputImage, outputImage];
    let options = slider.options;
    options.callback = function (obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);

    };

    slider = new juxtapose.JXSlider(transientSelector, images, options);
};



    (function () {
      slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
      //document.getElementById("left-button").onclick = replaceLeft;
      //document.getElementById("right-button").onclick = replaceRight;
    })();
    // Get the image text
    var imgText = document.getElementById("imgtext");
    // Use the same src in the expanded image as the image being clicked on from the grid
    // expandImg.src = imgs.src;
    // Use the value of the alt attribute of the clickable image as text inside the expanded image
    imgText.innerHTML = name;
    // Show the container element (hidden with CSS)
    // expandImg.parentElement.style.display = "block";

    $(".flip-card").click(function () {
      console.log("fading in")
      div_back = $(this).children().children()[1]
      div_front = $(this).children().children()[0]
      // console.log($(this).children("div.flip-card-back"))
      console.log(div_back)
      $(div_front).addClass("out");
      $(div_front).removeClass("in");

      $(div_back).addClass("in");
      $(div_back).removeClass("out");

    });

    $(".flip-card").mouseleave(function () {
      console.log("fading in")
      div_back = $(this).children().children()[1]
      div_front = $(this).children().children()[0]
      // console.log($(this).children("div.flip-card-back"))
      console.log(div_back)
      $(div_front).addClass("in");
      $(div_front).removeClass("out");

      $(div_back).addClass("out");
      $(div_back).removeClass("in");

    });

  </script>
  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js"
    integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>

</body>

</html>
