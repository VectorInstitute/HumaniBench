<!DOCTYPE html>
<html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta property='og:title' content='HumaniBench: A Benchmark for Human-Centric Alignment in Multimodal Large Language Models' />
<meta property='og:image' content='' />
<meta property='og:description' content='' />
<meta property='og:url' content='https://github.com/' />
<meta property='og:image:width' content='1200' />
<meta property='og:image:height' content='663' />
<!-- TYPE BELOW IS PROBABLY: 'website' or 'article' or look on https://ogp.me/#types -->
<meta property="og:type" content='website' />

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9VZKE74FPW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-9VZKE74FPW');
  </script>
  <meta charset="utf-8">
  <meta name="description" content="HumaniBench: A Benchmark for Human-Centric Alignment in Multimodal Large Language Models">
  <meta name="keywords" content="
    HumaniBench, Multimodal Large Language Models, Human-Centric Alignment, Benchmark, Dataset, Evaluation, Fairness, Ethics, Perceptual Honesty, Multilingual Equity, Empathy">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HumaniBench: A Benchmark for Human-Centric Alignment in Multimodal Large Language Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/tab_gallery.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/magnifier.js"></script>
  <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/image_card_fader.css">
  <link rel="stylesheet" href="./static/css/image_card_slider.css">

</head>

<style>
  @import url('https://fonts.cdnfonts.com/css/menlo');
</style>


<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h1 class="title is-2 publication-title">VLDBench: <u>V</u>ision <u>L</u>anguage Models -->
              <!-- <u>D</u>isinformation Detection <u>Bench</u>mark -->
            <h1 class="title is-2 publication-title">HumaniBench: A Benchmark for Human-Centric Alignment in Multimodal Large Language Models
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://scholar.google.com/citations?user=chcz7RMAAAAJ&hl=en">Shaina Raza</a><sup>1*</sup>,</span>
              <span class="author-block"><a href="https://aravind-3105.github.io/">Aravind Narayanan</a><sup>1*</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=lEWvRbIAAAAJ&hl=en">Vahid
                  Reza Khazaie</a><sup>1</sup></span>
              <span class="author-block"><a
                  href="https://scholar.google.com/citations?hl=en&user=Gn1FIg4AAAAJ">Mukund Sayeeganesh Chettiar</a><sup>1</sup></span>
              <span class="author-block"><a href="https://ashmalvayani.github.io/">Ashmal Vayani</a><sup>2</sup>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Vector Institute for Artificial Intelligence</span>
              <span class="author-block"><sup>2</sup>University of Central Florida</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://www.arxiv.org/abs/2502.11361" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/vector-institute/VLDBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
<!--                 <span class="link-block">
                  <a href="https://github.com/VectorInstitute" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
            
                <span class="link-block">
                  <a href="#bibtex" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-obp"></i>
                    </span>
                    <span>BibTex</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <p align="justify">
          As multimodal large language models (MLLMs) become increasingly integrated into our digital environments, their ability to align with human values, expectations, and ethics becomes critically important. We introduce <strong>HumaniBench</strong>, a minimalist yet comprehensive benchmark designed to evaluate MLLMs on their alignment with human-centric principles. Unlike traditional benchmarks that emphasize raw accuracy, HumaniBench probes a model’s fairness, ethical compliance, perceptual honesty, multilingual equity, and empathy. By presenting real-world, non-synthetic visual question answering tasks across diverse social contexts, HumaniBench offers a unique lens through which to assess how models interact with the nuances of human perception, identity, and culture. Our benchmark sets the stage for building MLLMs that are not just capable—but also conscientious and trustworthy.</p>
        <!-- <br> -->

        <div class="column">
          <div style="text-align:center;">
            <!-- <h4 class="subtitle has-text-centered"> -->
            <img src="static/images/Figure0.png" style="max-width:50%">
            <!-- </h4> -->

            <div class="content has-text-justified">
              <p align="justify"> <b> <span>Figure</span></b>:
                Overview of HumaniBench's alignment principles and associated evaluation metrics. The benchmark assesses models across seven dimensions of human-centered AI, including fairness, ethics, perceptual honesty, reasoning, multilinguality, empathy, and robustness.
              </p>
            </div>
          </div>
        </div>

        <br><br>
      </div>
    </div>

  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <!-- Visual Effects. -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal large language models (MLLMs) have achieved impressive capabilities in vision and language understanding; however, they often struggle on human-centered criteria such as <strong>fairness</strong>, <strong>ethical compliance</strong>, <strong>personalization</strong>, and <strong>perceptual accuracy</strong>—meaning their alignment with real-world human needs, values, and expectations. We introduce a <strong>novel benchmark dataset</strong> explicitly designed to evaluate and guide MLLMs toward more 
            <em>human-aligned behavior</em>. Specifically, we assess models’ ability to avoid <strong>harmful biases</strong>, ethically handle sensitive content, adapt outputs to user contexts, and maintain <em>human-like perceptual honesty</em> and <em>uncertainty handling</em>. We outline rigorous evaluation protocols that go beyond accuracy, measuring <strong>bias</strong>, <strong>trustworthiness</strong>, and <strong>user-centric performance</strong>. Baseline experiments with state-of-the-art MLLMs (open-source and proprietary) reveal significant gaps: models show strong performance at core vision-language tasks but still exhibit notable <strong>biases</strong> and <strong>consistency issues</strong> in human-centric scenarios. By highlighting these limitations, our benchmark provides a tool for the community to develop and validate MLLMs that are not only intelligent, but also <em>ethical</em>, <em>fair</em>, and <em>aligned with human perception and preferences</em>.<br>
          </p>
        </div>
      </div>
    </div>


  </section>




  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"><em>HumaniBench</em> is the first unified, real-world benchmark for evaluating human-centric alignment in multimodal LLMs; expert-verified across seven ethical AI dimensions.</h2>


          <div class="content has-text-justified">
            <p>
            <h5> <b> Main contributions: </b></h5>
            <ol>
              <li><strong>Unified Human-Centric Benchmark:</strong> Introduces HumaniBench, the first real-world benchmark that systematically evaluates multimodal LLMs across seven dimensions of human-centered AI, including fairness, empathy, and perceptual honesty.</li>
  
              <li><strong>Expert-Guided, Real-World Dataset:</strong> Comprises 32K image–question pairs derived from diverse social contexts and curated through a semi-automated GPT-4o pipeline with expert human validation—avoiding synthetic data to ensure realism.</li>
              
              <li><strong>Ethical & Multilingual Evaluation:</strong> Benchmarks models across multilingual equity, ethical compliance, and social sensitivity, offering robust metrics beyond accuracy (e.g., hallucination, empathy, contextual relevance, bias).</li>
              
              <li><strong>Holistic Model Assessment:</strong> Evaluates both open-source and proprietary MLLMs, revealing key alignment gaps and highlighting trade-offs between raw performance and human-centered trustworthiness.</li>

            </ol>
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

    </div>
  </section>

  <section class="section">

    <!--/ Matting. -->
    <div class="container is-max-desktop">

      <!-- Latent space editing applications -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          
          <h2 class="title is-3 has-text-centered">HumaniBench Framework Overview</h2>

          <div class="content has-text-justified">
            <p>VLDBench is a comprehensive classification multimodal benchmark for disinformation detection in news articles. We categorized our data into 13 unique news categories by providing image-text pairs to GPT-4o.</p>
            <div class="content has-text-centered"><img src="static/images/Table1.png"  style="max-width:80%">
              <p class="content has-text-justified"> <b> <span>Table</span></b>: Comparison of existing MLLM benchmarks with respect to seven human-centric alignment dimensions: (Fairness = lack of
                bias or stereotypes across demographics), (Ethical Compliance = adherence to moral or policy guidelines), (Perceptual Honesty = accurate
                interpretation of visual content without hallucination), (Contextual Reasoning = multi-turn or context-rich understanding), (Multilingual
                Equity = consistent performance across languages), (Empathy = sensitive responses to emotional or interpersonal scenarios), (Robustness
                = resilience to input perturbations or distribution shifts). Columns are marked with ✓ if covered, ✗ if not, and ∼ if partially. The “HC”
                column indicates overall human-centric coverage, and “Data Source” shows whether images are real (R) or synthetic (S).</p>
            </div>
          </div>


        </div>
      </div>

      <!--/ Matting. -->
      <div class="container is-max-desktop">

        <!-- Latent space editing applications -->
        <div class="columns is-centered">
          <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Annotation Pipeline</h2>
            <div class="content has-text-centered">
              <img src="static/images/Figure2.png">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Overview of the annotation process for HumaniBench
              </p>
            </div>

            <div class="content has-text-justified">
              <p align="justify"> 
                HumaniBench employs a multi-stage annotation pipeline combining automation and expert validation. Real-world images are first filtered for duplicates using CLIP embeddings, then tagged with captions and demographic labels via GPT-4o. These annotations are reviewed by domain experts to ensure accuracy, fairness, and appropriateness. Images are then clustered by attributes like age, race, and gender to support bias analysis. Final task-specific annotations are generated and verified, resulting in a high-quality dataset of 32K images with rich metadata across five social dimensions.
              </p>
          </div>


          <h2 class="title is-3 has-text-centered">Tasks Overview</h2>
            <div class="content has-text-centered">
              <img src="static/images/Table2.png" style="max-width:80%">
              <p class="content has-text-justified"><b> <span>Figure</span></b>: Overview of HumaniBench Tasks Grouped by Alignment Dimension.</p>
            </div>
            

            <div class="content has-text-justified">
              <p align="justify"> 
                Each of the seven tasks in HumaniBench corresponds to one or more of the seven core human-centric principles that we defined and is designed to reflect realistic, complex, and diverse scenarios.
              </p>
              <ul>
                <li><strong>T1: Scene Understanding</strong><br>
                    Open-ended VQA task where models describe a visual scene with attention to social context. Evaluates fairness and potential biases in describing individuals based on appearance, such as gender, race, and age.
                </li>
              
                <li><strong>T2: Instance Identity</strong><br>
                    Requires models to identify the key person or object in an image and interpret their role or identity. Assesses culturally sensitive reasoning and accuracy across social contexts.
                </li>
              
                <li><strong>T3: Instance Attribute</strong><br>
                    Multiple-choice questions about object-specific visual traits (e.g., color, accessories, attire). Measures perceptual honesty, hallucination, and subtle detail recognition.
                </li>
              
                <li><strong>T4: Language & Culture</strong><br>
                    Multilingual VQA task featuring questions in 11 languages to test cross-lingual consistency and cultural reasoning. Targets equity in MLLM performance across high- and low-resource languages.
                </li>
              
                <li><strong>T5: Visual Grounding & Localization</strong><br>
                    Models are prompted to localize specific entities in images using bounding boxes. Tasks emphasize spatial reasoning and visual-text grounding accuracy.
                </li>
              
                <li><strong>T6: Human-Centered Emotion</strong><br>
                    Assesses the ability of models to rewrite factual captions into emotionally sensitive responses for crisis or distress scenarios. Focuses on empathy and tone appropriateness.
                </li>
              
                <li><strong>T7: Robustness & Stability</strong><br>
                    Evaluates how model responses change under image perturbations (e.g., blur, noise). Tests the consistency and resilience of MLLM outputs under stress conditions.
                </li>
              </ul>
              
          </div>

        </div>
      </div>
      <br><br>
      <div class="content has-text-centered">
        <h3 class="title is-4 has-text-justified">LLMs and VLMs used for Benchmarking</h3>
        <div class="content has-text-justified">
          <p align="justify"> For HumaniBench, we evaluate thirteen state-of-the-art open-source multimodal models (VLMs) across diverse human-alignment tasks. These models represent a range of vision encoders, language backbones, and fusion techniques. By focusing exclusively on vision-language models, HumaniBench provides a rigorous testbed for assessing human-centered reasoning, fairness, empathy, and robustness in real-world multimodal scenarios. </p>
          <!-- I want width to be 80% center aligned-->
          <table style="width:60%; margin: 0 auto; border-collapse: collapse; font-family: sans-serif;">
            <thead style="background-color: #f2f2f2;">
              <tr>
                <th style="padding: 12px; border-bottom: 2px solid #ccc; font-size: 1.1em; text-align: left;">
                  Vision-Language Models (VLMs)
                </th>
              </tr>
            </thead>
            <tbody>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">CogVLM2-19B</td></tr>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Cohere Aya Vision 8B</td></tr>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">DeepSeek VL2 Small</td></tr>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">GLM-4V-9B</td></tr>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">InternVL2.5</td></tr>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Janus-Pro-7B</td></tr>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">LLaMA-3.2-11B-Vision</td></tr>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">LLaVA-v1.6</td></tr>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Molmo-7B</td></tr>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Phi 4 Multimodal</td></tr>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Phi 3.5 Vision</td></tr>
              <tr><td style="padding: 10px; border-bottom: 1px solid #e0e0e0;">Qwen2.5-VL-7B</td></tr>
              <tr><td style="padding: 10px;">Gemma 3</td></tr>
            </tbody>
          </table>
        
        
      </div>

      <br><br>

      <div class="columns is-centered">
        <div class="column is-full-width has-text-centered">
          <h2 class="title is-3">Experimental results on VLDBench</h2>

          <div class="content has-text-justified">
            <p> Our investigation focuses on three core ques404 tions: (1) Does multimodal (text+image) data improve
              disinformation detection compared to text alone? (2) Does instruction-based fine tuning enhance
              generalization and robustness? (3) How vulnerable are models to adversarial perturbations in text and
              images? </p>
          </div>

          <h3 class="title is-4 has-text-justified">Multimodal Models Surpass Unimodal Baselines</h3>

          <div class="content has-text-centered">
            <p style="text-align: justify;">
              VLMs generally out perform language-only LLMs. For example, LLaMA-3.2-11B-Vision outperforms LLaMA 3.2-1B (text-only). Similarly, Phi, LLaVA, Pixtral, InternVL, DeepSeek-VL, and GLM360 4V perform better than their text-only counterparts. The performance gains between these two sets of models are quite pronounced. For instance, LLaVA-v1.5-Vicuna7B improves accuracy by 27% over its unimodal base (Vicuna-7B), highlighting the critical role of visual context. However, Qwen2-VL-7B marginally lags behind its text-only counterpart, suggesting that the effectiveness of modality integration can vary depending on the model’s architecture. While top LLMs remain competitive, VLMs excel in recall, a vital trait for minimizing missed disinformation in adversarial scenarios.</p>
            <!-- <img src="static/images/zero-shot_performance.png" style="max-width:100%"> -->
             <!-- <img src="static/images/radar_plot_page-0001.jpg" style="max-width:80%"> -->
            <!-- <p style="text-align: justify;"> <b> <span>Figure</span></b>: Performance comparison of vision language models for disinformation detection across key metrics: precision, recall, F1 and accuracy, with different colors representing distinct metrics </p> -->
          </div>
          <!-- <br /> -->
          <h3 class="title is-4 has-text-justified">Instruction Fine-Tuning Enhances Performance</h3>
          <div class="content has-text-centered">
            <p style="text-align: justify;">
              Instruction fine-tuning (IFT) on models like Phi, Mistral-Llava, Qwen, and Llama3.2—along with their vision-language counterparts—utilizing the training subset of VLDBench led to significant performance improvements across all models compared to their zero-shot capabilities. For instance, Phi-3-Vision-IFT achieved a 7% increase in F1 model over their zero-shot baselines. This enhancement is not solely due to better output formatting; rather, it reflects the model ability to adapt to and learn from disinformation-specific cues in the data.</p>
            <img src="static/images/ift.png" style="max-width:80%">
            <p style="text-align: justify;"> <span><b>Figure</b></span>: Comparison of zero-shot vs. instruction-fine-tuned (IFT) performance, with 95% confidence intervals computed from three independent runs. </p>


          </div>
        </div>
      </div>

      <h2 class="title is-3">Robustness to Adversarial Perturbations</h2>
      <h3 class="title is-4 has-text-justified">Text and Image Attacks</h3>

      <div class="content has-text-justified">
        <p> We tested each model under controlled perturbations (zero-shot evaluation), including textual and image perturbations. Textual perturbations include synonym substitution, misspellings, and negations. Image perturbations (I-P) include blurring, gaussian noise, and resizing. We also include Multi modality attacks: cross-modal misalignment (C-M) (e.g., mismatched image captions) and both modality perturbations (B-P) (both text and image distortions). The following images are examples of various perturbations.</p>
        <div class="content has-text-centered">
          <img src="static/images/perturbation_text_page-0001.jpg" style="max-width:65%">
          <p style="text-align: justify;"> <b> <span>Figure</span></b>: We describe the text perturbations in the caption,
            introducing Synonym, Misspelling, and Negation. Our analysis shows that text negation leads to a majority of
            disinformation cases.</p>
        </div>

          
        <!-- <p>Image perturbations (I-P) include blurring, gaussian noise, and resizing. We also include Multi modality attacks: cross-modal misalignment (C-M) (e.g., mismatched image captions) and both modality perturbations (B-P) (both text and image distortions). The following image is an example of image, cross-modal, and both-modality perturbations.
        </p> -->
        <div class="content has-text-centered">
          <img src="static/images/perturbation_image_page-0001.jpg" style="max-width:65%">
          <p style="text-align: justify;"> <b> <span>Figure</span></b>: We describe the image perturbations in the caption, introducing Blur, Noise, and Resizing, Cross-Modal (C-M) Mismatch, Both-Modality (BM). Our analysis shows that C-M and B-M leads to a majority of disinformation cases.</p>
        </div>
        
      </div>
     

      
      <h3 class="title is-4 has-text-justified">Combined Attacks</h3>

      <div class="content has-text-justified">
        <p> Combining text+image adversarial attacks can cause catastrophic performance drops in high-capacity models. These findings illustrate that multimodal methods, despite generally higher baseline accuracy, remain susceptible when adversaries deliberately target both modalities.</p>
      </div>
      <!-- <div class="content has-text-centered">
        <img src="static/images/VisualTransformations.png" style="max-width:100%">
        <p style="text-align: justify;"> <b> <span>Figure</span></b>: Visual transformations and their impact on disinformation classification. Noise and downscaling can shift model outputs from “Yes” to “No.”</p>
      </div> -->

      <h3 class="title is-4 has-text-justified">Human Evaluation Establishes Reliability and Reasoning Depth</h3>

      <div class="content has-text-justified">
        <p> We conducted a human evaluation of three IFT VLMs (LlaMA-3.2-11B, Pixtral, LLaVA-v1.6) on a balanced 500-sample test set (250 disinformation, 250 neutral). Each model classified samples and provided a rationale. Three independent reviewers, blinded to model identities, rated the outputs based on Prediction Correctness (PC) and Reasoning Clarity (RC), both on a scale of 1–5. The image below shows a representative example of model reasoning and highlights differences in explanatory quality.</p>
      </div>
      <div class="content has-text-centered">
        <img src="static/images/reasoning_page-0001.jpg" style="max-width:80%">
        <p style="text-align: justify;"> <b> <span>Figure</span></b>: Human evaluation results on a 500-sample test set. Models were tasked with classifying disinformation and justifying their predictions. PC = prediction correctness, RC = reasoning clarity (mean ± std.).</p>
      </div>

      <div class="content has-text-justified">
        <h3 class="title is-4 has-text-justified">Conclusion</h3>
        <div class="content has-text-justified">
          <p>
            VLDBench addresses the urgent challenge of disinformation through a design rooted in responsible data stewardship
            and human-centered principles, integrating best practices to meet key AI governance requirements. Unlike other
            benchmarks, VLDBench uniquely targets the complexity of disinformation in the post-ChatGPT era, where GenAI has
            amplified a lot of false information. It is the first dataset explicitly designed to evaluate modern V/LLMs on
            emerging disinformation challenges, maintaining a topical focus.
            <br><br>
            However, some limitations need attention. The reliance on pre-verified news sources introduces potential sampling
            bias, and the annotation process, partially based on AI, may inherit some biases. There is also a need for more
            research into adversarial attacks on multimodal performance. The current focus on English language only limits its applicability to multilingual and culturally diverse contexts. Despite these limitations, VLDBench represents an effort in benchmarking disinformation detection and opens venues for collaboration from researchers and practitioners to address this challenge.</p>
          <br>
          <p>For additional details about VLDBench evaluation and experimental results, please refer to our main paper.
            Thank you! </p>
        </div>

        <div class="columns is-centered">
          <div class="column is-full-width has-text-centered">
            <h2 class="title is-3">Social Statement</h2>
          </div>
        </div>
        <div class="content has-text-justified">
          <p> Disinformation threatens democratic institutions, public trust, and social cohesion. <b>Generative AI
              exacerbates the problem</b> by enabling sophisticated multimodal campaigns that exploit cultural,
            political, and linguistic nuances, requiring solutions beyond technical approaches. </p>
          <p> <b>VLDBench</b> addresses this challenge as the first multimodal benchmark for disinformation detection,
            combining text and image analysis with ethical safeguards. It prioritizes <b>cultural sensitivity</b>
            through regional annotations and mitigates bias with audits and human-AI hybrid validation. Ground-truth
            labels are sourced from fact-checked references with transparent provenance tracking. </p>
          <p> As both a <b>technical resource</b> and a <b>catalyst for collaboration</b>, VLDBench democratizes
            access to cutting-edge detection tools by open-sourcing its benchmark and models. It highlights systemic
            risks, like adversarial attack vulnerabilities, to drive <b>safer and more reliable systems</b>. Designed
            to foster partnerships across academia, industry, journalism, and policymaking, VLDBench bridges the gap
            between research and real-world impact. </p>
          <p> <b>Ethical risks</b> are carefully addressed through restricted access, exclusion of synthetic tools,
            and human oversight requirements. Representation gaps in non-English content are documented to guide
            future adaptations. Binding agreements prohibit harmful applications such as censorship, surveillance, or
            targeted disinformation campaigns. </p>
          <p> By focusing exclusively on disinformation detection, VLDBench supports <b>media literacy</b>, unbiased
            fact-checking, and policy discussions on AI governance. Its <b>ethical design</b> and <b>equitable
              access</b> empower communities and institutions to combat disinformation while fostering trust in
            digital ecosystems. </p>
        </div>
      </div>
    </div>



      

  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title"><a id="bibtex">BibTeX</a></h2>
      <pre><code>To be released.</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p class="has-text-centered">
            Website adapted from the following <a href="https://mingukkang.github.io/GigaGAN/">source code</a>.
          </p>
        </div>
      </div>
    </div>
    </div>
  </footer>


  <script src="juxtapose/js/juxtapose.js"></script>

  <script>
    var slider;
    let origOptions = {
      "makeResponsive": true,
      "showLabels": true,
      "mode": "horizontal",
      "showCredits": true,
      "animate": true,
      "startingPosition": "50"
    };

    const juxtaposeSelector = "#juxtapose-embed";
    const transientSelector = "#juxtapose-hidden";

    inputImage.src = "./static/images/".concat(name, "_input.jpg")
    outputImage.src = "./static/images/".concat(name, "_output.jpg")

    let images = [inputImage, outputImage];
    let options = slider.options;
    options.callback = function (obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);

    };

    slider = new juxtapose.JXSlider(transientSelector, images, options);
};



    (function () {
      slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
      //document.getElementById("left-button").onclick = replaceLeft;
      //document.getElementById("right-button").onclick = replaceRight;
    })();
    // Get the image text
    var imgText = document.getElementById("imgtext");
    // Use the same src in the expanded image as the image being clicked on from the grid
    // expandImg.src = imgs.src;
    // Use the value of the alt attribute of the clickable image as text inside the expanded image
    imgText.innerHTML = name;
    // Show the container element (hidden with CSS)
    // expandImg.parentElement.style.display = "block";

    $(".flip-card").click(function () {
      console.log("fading in")
      div_back = $(this).children().children()[1]
      div_front = $(this).children().children()[0]
      // console.log($(this).children("div.flip-card-back"))
      console.log(div_back)
      $(div_front).addClass("out");
      $(div_front).removeClass("in");

      $(div_back).addClass("in");
      $(div_back).removeClass("out");

    });

    $(".flip-card").mouseleave(function () {
      console.log("fading in")
      div_back = $(this).children().children()[1]
      div_front = $(this).children().children()[0]
      // console.log($(this).children("div.flip-card-back"))
      console.log(div_back)
      $(div_front).addClass("in");
      $(div_front).removeClass("out");

      $(div_back).addClass("out");
      $(div_back).removeClass("in");

    });

  </script>
  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js"
    integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>

</body>

</html>
